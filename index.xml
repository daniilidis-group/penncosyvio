<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PennCOSYVIO Data Set</title>
    <link>https://daniilidis-group.github.io/penncosyvio/</link>
    <description>Recent content on PennCOSYVIO Data Set</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Mar 2016 21:07:13 +0100</lastBuildDate>
    <atom:link href="https://daniilidis-group.github.io/penncosyvio/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The PennCOSYVIO Data Set</title>
      <link>https://daniilidis-group.github.io/penncosyvio/</link>
      <pubDate>Tue, 08 Mar 2016 21:07:13 +0100</pubDate>
      
      <guid>https://daniilidis-group.github.io/penncosyvio/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;pics/trajectory_animation.gif&#34; alt=&#34;ground truth from markers&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;what-is-penncosyvio&#34;&gt;What is PennCOSYVIO?&lt;/h2&gt;

&lt;p&gt;The PennCOSYVIO data set is collection of synchronized video and IMU data recorded at the University of Pennsylvania&amp;rsquo;s Singh Center in April 2016. It is geared towards benchmarking of Visual Inertial Odometry algorithms on hand-held devices, but can also be used for other platforms such as micro aerial vehicles or ground robots.&lt;/p&gt;

&lt;h2 id=&#34;trajectory&#34;&gt;Trajectory&lt;/h2&gt;

&lt;p&gt;What sets this benchmark apart from previous ones is that it goes from outdoors to indoors:&lt;br&gt;
&lt;img src=&#34;pics/singh_outdoors_gopro.jpg&#34; alt=&#34;Singh Center from the outside&#34; /&gt;&lt;br&gt;
&lt;img src=&#34;pics/singh_indoors_tango_rgb.jpg&#34; alt=&#34;inside the Singh Center&#34; /&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;The total path length is about 150m. We use an optical method to localize the sensors via fiducial markers (AprilTags) to within about 10cm. The animation at the top of the page shows which markers along the path are visible. These &amp;ldquo;ground truth&amp;rdquo; positions can be used to benchmark the results of VIO algorithms such as the Google Tango&amp;rsquo;s:
&lt;img src=&#34;pics/sequence_as.jpg&#34; alt=&#34;sequence AS trajectory&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sensors&#34;&gt;Sensors&lt;/h2&gt;

&lt;p&gt;We loaded a bunch of sensors onto the rig: seven cameras and three IMUs total, including two Google Project Tango tablets, four GoPro Hero 4 Cameras, and a VI (Visual-Inertial) sensor.
Nitin then hauled the rig into and through the Singh center:&lt;br&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&#34;pics/rig.jpg&#34; width=&#34;500&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;pics/rig_carried.jpg&#34; width=&#34;600&#34;&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;&lt;/p&gt;

&lt;p&gt;Here are the sensor characteristics:
&lt;table&gt;
&lt;tr&gt;&lt;td&gt;Sensor&lt;/td&gt;&lt;td&gt;Characteristics&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;C1,C2,C3&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;&lt;li&gt;GoPro Hero 4 Black&lt;/li&gt;
&lt;li&gt;&lt;RGB 1920x1080 at 30fps on &#39;W&#39; (wide) setting&lt;/li&gt;
&lt;li&gt;rolling shutter&lt;/li&gt;
&lt;li&gt;FOV: 69.5deg vert., 118.2deg horiz&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VI-Sensor&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;Skybotix integrated VI-sensor&lt;/li&gt;
&lt;li&gt;stereo camera: 2 x Aptina MT9V034&lt;/li&gt;
&lt;li&gt;gray 2x752x480 at 20fps (rectified), global shutter&lt;/li&gt;
&lt;li&gt;FOV: 57deg vert., 2 x 80deg horiz.&lt;/li&gt;
&lt;li&gt;IMU: ADIS16488 at 200Hz&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tango Bottom&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;Google Project Tango &amp;lsquo;Yellowstone&amp;rsquo; 7in tablet&lt;/li&gt;
&lt;li&gt;RGB 1920x1080 at 30fps, rolling shutter&lt;/li&gt;
&lt;li&gt;FOV: 31deg vert., 52deg horiz.&lt;/li&gt;
&lt;li&gt;proprietary VIO pose estimation&lt;/li&gt;
&lt;li&gt;accelerometer at 128Hz&lt;/li&gt;
&lt;li&gt;gyroscope at 100Hz&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tango Top&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;Google Project Tango &amp;lsquo;Yellowstone&amp;rsquo; 7in tablet&lt;/li&gt;
&lt;li&gt;gray 640x480 at 30fps, global shutter&lt;/li&gt;
&lt;li&gt;FOV: 100deg vert., 132deg horiz.&lt;/li&gt;
&lt;li&gt;accelerometer at 128Hz&lt;/li&gt;
&lt;li&gt;gyroscope at 100Hz&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;/p&gt;

&lt;h2 id=&#34;citations&#34;&gt;Citations&lt;/h2&gt;

&lt;p&gt;If you are using this dataset, please cite the &lt;a href=&#34;docs/penncosyvio.pdf&#34;&gt;following publication&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@inproceedings{DBLP:conf/icra/PfrommerSDC17,
author    = {Bernd Pfrommer and
Nitin Sanket and
Kostas Daniilidis and
Jonas Cleveland},
title     = {PennCOSYVIO: {A} challenging Visual Inertial Odometry benchmark},
booktitle = {2017 {IEEE} International Conference on Robotics and Automation, {ICRA}
2017, Singapore, Singapore, May 29 - June 3, 2017},
pages     = {3847--3854},
year      = {2017},
crossref  = {DBLP:conf/icra/2017},
url       = {https://doi.org/10.1109/ICRA.2017.7989443},
doi       = {10.1109/ICRA.2017.7989443},
timestamp = {Wed, 26 Jul 2017 15:17:30 +0200},
biburl    = {http://dblp.uni-trier.de/rec/bib/conf/icra/PfrommerSDC17},
bibsource = {dblp computer science bibliography, http://dblp.org}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Benchmarking</title>
      <link>https://daniilidis-group.github.io/penncosyvio/benchmarking/</link>
      <pubDate>Wed, 09 Mar 2016 00:11:02 +0100</pubDate>
      
      <guid>https://daniilidis-group.github.io/penncosyvio/benchmarking/</guid>
      <description>

&lt;h2 id=&#34;compiling-and-using-the-benchmarking-tools&#34;&gt;Compiling and using the benchmarking tools&lt;/h2&gt;

&lt;p&gt;The benchmarking tool uses the c++ and GSL library. On Ubuntu, the GSL
library can be installed via &lt;code&gt;sudo apt-get install gsl-bin
libgs10-dev&lt;/code&gt;, and obviously you need the g++ compiler and &lt;code&gt;make&lt;/code&gt; to be
installed. To compile the code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nohighlight&#34;&gt;cd tools/cpp
make
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This should produce (among others) the &lt;code&gt;test_trajectory&lt;/code&gt; executable,
which you can use to evaluate the error of a trajectory. As an
example, here is how to use the program (&lt;code&gt;test_trajectory -h&lt;/code&gt; gives
options to allow scaling etc) to test the output from Tango Bottom&amp;rsquo;s
proprietary VIO algorithm. You need to download the Tango Bottom data
for that to work.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nohighlight&#34;&gt;./test_trajectory -r ../../data/ground_truth/af/pose.txt -t ../../data/tango_bottom/af/pose.txt
read 2885 poses from ../../data/ground_truth/af/pose.txt
read 9596 poses from ../../data/tango_bottom/af/pose.txt
*not* scaling by scale 1.1144
alignment transform: 
[               1  1.24075825e-05 -1.12979008e-05  -0.00269686406
   -1.2406981e-05     0.999999999  5.32406084e-05  -0.00113532707
   1.12985614e-05 -5.32404682e-05     0.999999999  0.000683608497]

---------------------------- 
length of reference trajectory: 148.844m
length of test trajectory:      169.189m
---------------------------- 
--- Average Trajectory Error:
 rotational error: 1.2967deg
 error in x:  0.715m
 error in y:  0.172m
 error in z:  1.753m
 2d error in x/y plane:  0.908m
 3d positional error:    2.497m
--- Relative Position Error:
 rotational error: 1.5382deg
 error in x:  2.812%
 error in y:  1.203%
 error in z:  8.864%
 2d error in x/y plane:  4.746%
 3d positional error:   13.114%
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;visualizing-results&#34;&gt;Visualizing results&lt;/h2&gt;

&lt;p&gt;Matlab scripts to visualize trajectories are provided in the matlab
directory. They take files as input that have pose sequences
(trajectories) in the standard format used throughout. Here is how to
visualize the Tango poses vs the ground truth in Matlab:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color: #f8f8f2&#34;&gt;cd&lt;/span&gt; &lt;span style=&#34;color: #f8f8f2&#34;&gt;tools&lt;/span&gt;&lt;span style=&#34;color: #f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;matlab&lt;/span&gt;
&lt;span style=&#34;color: #f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color: #f8f8f2&#34;&gt;p1&lt;/span&gt; &lt;span style=&#34;color: #f8f8f2&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color: #f8f8f2&#34;&gt;read_trajectory(&lt;/span&gt;&lt;span style=&#34;color: #e6db74&#34;&gt;&amp;#39;../../data/ground_truth/as/pose.txt&amp;#39;&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;);&lt;/span&gt;
&lt;span style=&#34;color: #f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color: #f8f8f2&#34;&gt;p2&lt;/span&gt; &lt;span style=&#34;color: #f8f8f2&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color: #f8f8f2&#34;&gt;read_trajectory(&lt;/span&gt;&lt;span style=&#34;color: #e6db74&#34;&gt;&amp;#39;../../data/tango_bottom/as/pose.txt&amp;#39;&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;);&lt;/span&gt;
&lt;span style=&#34;color: #f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color: #f8f8f2&#34;&gt;view_trajectories({p1,p2},&lt;/span&gt; &lt;span style=&#34;color: #e6db74&#34;&gt;&amp;#39;../misc/tags.txt&amp;#39;&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The result should be a picture like this:&lt;br/&gt;
&lt;img src=&#34;../pics/view_trajectories.jpg&#34;&gt;&lt;br/&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Download</title>
      <link>https://daniilidis-group.github.io/penncosyvio/download/</link>
      <pubDate>Wed, 09 Mar 2016 00:11:02 +0100</pubDate>
      
      <guid>https://daniilidis-group.github.io/penncosyvio/download/</guid>
      <description>

&lt;h2 id=&#34;preparing-for-download&#34;&gt;Preparing for Download&lt;/h2&gt;

&lt;p&gt;The installation instructions and scripts have been tested on
Ubuntu 14.04. Porting the scripts to any unix-like operating system
should be straight forward. The Windows OS is not supported, but we
provide raw download links so you can &lt;a href=&#34;#manual_download&#34;&gt;piece the data together
yourself&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The full data set, without the frames extracted, has a size of about 25GB. If frames are extracted for all cameras and all sequences, the total size balloons to more than 100GB, so having some head room is definitely a good idea. For most purposes, 50GB should be more than enough space.&lt;/p&gt;

&lt;h2 id=&#34;running-the-installer&#34;&gt;Running the Installer&lt;/h2&gt;

&lt;p&gt;The install scripts are in the &amp;ldquo;master&amp;rdquo; branch of the
github repository, and can be run like this (you may need to install
&lt;code&gt;git&lt;/code&gt; if it&amp;rsquo;s not on your machine yet, using &lt;code&gt;sudo apt-get install git&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nohighlight&#34;&gt;git clone https://github.com/daniilidis-group/penncosyvio.git
cd penncosyvio/tools
./download.bash 
================ PENNCOSYVIO INSTALLER ======================
------------ select sensor to download:
1) GoPro C1		  4) Tango Bottom	    7) Intrinsic Calibration
2) GoPro C2		  5) Tango Top		    8) Extrinsic Calibration
3) GoPro C3		  6) VI Sensor		    9) Quit
enter your choice: 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pick the sensors that you are interested in and download the
data, which will end up in the &lt;code&gt;data&lt;/code&gt; subdirectory. Once you quit out
of the installer tool, change into the data directory and see what&amp;rsquo;s
there. This is how it looks if you downloaded only the &lt;code&gt;Tango Top&lt;/code&gt; data:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nohighlight&#34;&gt;cd ..
tree data
data
└── tango_top
    ├── af
    │   ├── accelerometer.txt
    │   ├── gyroscope.txt
    │   ├── seq_af.mp4
    │   └── timestamps.txt
    ├── as
    │   ├── accelerometer.txt
    │   ├── gyroscope.txt
    │   ├── seq_as.mp4
    │   └── timestamps.txt
    ├── bf
    │   ├── accelerometer.txt
    │   ├── gyroscope.txt
    │   ├── seq_bf.mp4
    │   └── timestamps.txt
    └── bs
        ├── accelerometer.txt
        ├── gyroscope.txt
        ├── seq_bs.mp4
        └── timestamps.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;running-the-frame-extractor&#34;&gt;Running the Frame Extractor&lt;/h2&gt;

&lt;p&gt;Because reading mp4 files programmatically can be difficult (and notably because Matlab on Ubuntu skips frames due to some issues with the gstreamer framework), we provide a bash script to extract frames in PNG format. The script relies on either ffmpeg or avconv as an extraction tool. Either of them must be installed beforehand, avconf being easier to install as it is part of the Ubuntu distibution. Here is how to use the extraction tool:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nohighlight&#34;&gt;cd tools
./extract_frames.bash
------------ select frames to extract:
1) GoPro C1	 3) GoPro C3	  5) Tango Top
2) GoPro C2	 4) Tango Bottom  6) Quit
enter your choice: 5
------------- unpacking frames for Tango Top
 Be patient, this will take about 10-15min/sequence!
....
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The extracted frames will end up in the respective subdirectory of the &lt;code&gt;data&lt;/code&gt; subdirectory.&lt;/p&gt;

&lt;h2 id=&#34;manual-download&#34;&gt;Manual Download&lt;/h2&gt;

&lt;p&gt;&lt;a name=&#34;manual_download&#34;&gt;&lt;/a&gt;
Here are links for direct download of the data files:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://visiondata.cis.upenn.edu/penncosyvio/tarfiles/c1.tar&#34;&gt;GoPro C1 (29.4GB)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://visiondata.cis.upenn.edu/penncosyvio/tarfiles/c2.tar&#34;&gt;GoPro C2 (29.4GB)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://visiondata.cis.upenn.edu/penncosyvio/tarfiles/c3.tar&#34;&gt;GoPro C3 (29.5GB)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://visiondata.cis.upenn.edu/penncosyvio/tarfiles/tango_bottom.tar&#34;&gt;Tango Bottom (9.2GB)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://visiondata.cis.upenn.edu/penncosyvio/tarfiles/tango_top.tar&#34;&gt;Tango Top (2.7GB)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://visiondata.cis.upenn.edu/penncosyvio/tarfiles/visensor.tar&#34;&gt;VI Sensor (10.5GB)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After download, untar them in the &lt;code&gt;data&lt;/code&gt; directory.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>File Format</title>
      <link>https://daniilidis-group.github.io/penncosyvio/file_format/</link>
      <pubDate>Wed, 09 Mar 2016 00:11:02 +0100</pubDate>
      
      <guid>https://daniilidis-group.github.io/penncosyvio/file_format/</guid>
      <description>

&lt;h2 id=&#34;video-files&#34;&gt;Video Files&lt;/h2&gt;

&lt;p&gt;Raw video files are provided as &lt;code&gt;mp4&lt;/code&gt; files, with corresponding
&lt;code&gt;timestamps.txt&lt;/code&gt; files. For the VI sensor, we recommend using the
original data from the ROS bag or the extracted frames rather than the
provided &lt;code&gt;mp4&lt;/code&gt; files as the latter suffer from quality loss due to
compression.&lt;/p&gt;

&lt;p&gt;See the description in the &lt;a href=&#34;../download&#34;&gt;Download&lt;/a&gt; section on how to extract
individual frames from the mp4 files. Note
that a bug somewhere between gstreamer and Matlab makes playing back
frames with Matlab&amp;rsquo;s VideoReader unreliable. It is recommended to play
back from the extracted frames using the &lt;code&gt;CustomVideoReader&lt;/code&gt; Matlab
class in the &lt;code&gt;tools/matlab/&lt;/code&gt; directory.&lt;/p&gt;

&lt;p&gt;For the orientation of the camera axis, see the &lt;a href=&#34;../extrinsic_calib&#34;&gt;Extrinsic Calibration&lt;/a&gt; section.&lt;/p&gt;

&lt;h2 id=&#34;imu-files&#34;&gt;IMU Files&lt;/h2&gt;

&lt;p&gt;The IMU data is stored in flat text files:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;accelerometer.txt&lt;/code&gt; (Tango): timestamp in seconds, acceleration in x,y,z
(in m/sec^2)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gyroscope.txt&lt;/code&gt; (Tango): timestamp in seconds, angular velocity around x,
y, z axes (in rad/sec)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;imu.txt&lt;/code&gt; (VI sensor): timestamp in seconds, acceleration in x,y,z
(in m/sec^2), angular velocity around x, y, z axes (in rad/sec)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For axis orientation of the IMUs, please refer to the &lt;a href=&#34;../extrinsic_calib&#34;&gt;Extrinsic Calibration&lt;/a&gt; section.&lt;/p&gt;

&lt;h2 id=&#34;pose-ground-truth-files&#34;&gt;Pose (Ground Truth) Files&lt;/h2&gt;

&lt;p&gt;The ground truth and Tango pose files are flat text files that contain the pose as a
3x4 matrix
$$
\cc{\mvec{P}:=\begin{bmatrix}\mvec{R}\ \mvec{t}\end{bmatrix}}
$$
corresponding to the body(rig)-to-world transform:
$$
\cc{\ctrans{T}{B}{W} = \begin{bmatrix}\mvec{R}\ \mvec{t}\end{bmatrix}}
$$
Each line in the pose file starts with a timestamp (in seconds) and is
followed by the matrix $\cc{\mvec{P}}$ in row major format, i.e:&lt;/p&gt;

&lt;p&gt;timestamp(sec),
$\cc{\mvec{P}_{11}}$, $\cc{\mvec{P}_{12}}$,$\cc{\mvec{P}_{13}}$, $\cc{\mvec{P}_{14}}$,
$\cc{\mvec{P}_{21}}$, $\cc{\mvec{P}_{22}}$,$\cc{\mvec{P}_{23}}$, $\cc{\mvec{P}_{24}}$,
$\cc{\mvec{P}_{31}}$, $\cc{\mvec{P}_{32}}$,$\cc{\mvec{P}_{33}}$, $\cc{\mvec{P}_{34}}$.&lt;/p&gt;

&lt;p&gt;Input files for benchmarking must be produced in this format as well.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intrinsic Calibration</title>
      <link>https://daniilidis-group.github.io/penncosyvio/intrinsic_calib/</link>
      <pubDate>Wed, 09 Mar 2016 00:11:02 +0100</pubDate>
      
      <guid>https://daniilidis-group.github.io/penncosyvio/intrinsic_calib/</guid>
      <description>

&lt;p&gt;Two different models were used for the intrinsic calibration of the cameras:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mathworks.com/help/vision/ug/camera-calibration.html&#34;&gt;standard perspective&lt;/a&gt;
model with two radial distortion distortion coefficients.
This model works well for the Tango Bottom RGB camera and the VI sensor cameras&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/scarabotix/ocamcalib-toolbox&#34;&gt;omnidirectional model&lt;/a&gt; for
the GoPro cameras and the Tango Top. This is necessary to accurately
model the wide-angle lenses projection function all the way to the corners of the sensor.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A square chessboard calibration target of 7x8 was used with square
length of 108mm. The images were cut from video, and are available
under the &lt;code&gt;intrinsic_calibration&lt;/code&gt; directory (requires separate download).&lt;/p&gt;

&lt;h2 id=&#34;how-to-use-the-calibration-models&#34;&gt;How to Use the Calibration Models&lt;/h2&gt;

&lt;p&gt;&lt;a name=&#34;howtousecalibmodels&#34;&gt;&lt;/a&gt;
The camera calibration models are stored under &lt;code&gt;tools/intrinsic_calibration/cc.mat&lt;/code&gt; and can be loaded
in Matlab like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nohighlight&#34;&gt;&amp;gt;&amp;gt; cd tools/matlab;
&amp;gt;&amp;gt; load ../intrinsic_calibration/cc.mat
&amp;gt;&amp;gt; cc

cc = 

    [1x1 struct]    [1x1 struct]    [1x1 struct]    [1x1 cameraParameters]    [1x1 struct]    [1x1 cameraParameters]    [1x1 cameraParameters]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;cc&lt;/code&gt; cell array has the cameras in the order:
&lt;table&gt;
&lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;GoPro C1&lt;/td&gt;&lt;td&gt;GoPro C2&lt;/td&gt;&lt;td&gt; GoPro C3&lt;/td&gt;&lt;td&gt; Tango Bottom&lt;/td&gt;&lt;td&gt; Tango Top&lt;/td&gt;&lt;td&gt;VI Sensor Left&lt;/td&gt;&lt;td&gt;VI Sensor Right&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;&lt;/p&gt;

&lt;h3 id=&#34;undistorting-tango-bottom-and-vi-sensor-images&#34;&gt;Undistorting Tango Bottom and VI Sensor images&lt;/h3&gt;

&lt;p&gt;To undistort images for Tango Bottom&amp;rsquo;s RGB camera or the VI Sensors, you can directly use the Matlab &lt;code&gt;undistortImage&lt;/code&gt; function. The following matlab
line would read an image, undistort it with the Tango Bottom calibration parameters, and display it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nohighlight&#34;&gt;&amp;gt;&amp;gt; imtool(undistortImage(imread(&#39;foo.jpg&#39;),cc{4}));
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;undistorting-gopro-and-tango-top-images&#34;&gt;Undistorting GoPro and Tango Top Images&lt;/h3&gt;

&lt;p&gt;&lt;a name=&#34;undistfisheye&#34;&gt;&lt;/a&gt;
Images taken with the fisheye lenses require the use of a custom undistortion matlab function for undistortion.
Undistortion is a two-step process: first a undistortion map is
pre-computed (this needs to be done only once per camera),
which then is used to perform the actual undistortion. For example, this would be the steps to undistort an image
for GoPro camera C1:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nohighlight&#34;&gt;&amp;gt;&amp;gt; u = ocam_undistort_map(cc{1}, &#39;OutputView&#39;, &#39;full&#39;);
&amp;gt;&amp;gt; imtool(ocam_undistort(imread(&#39;../../intrinsic_calibration/c1/frame_0011.png&#39;), u));
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img alt=&#34;distorted original&#34; src=&#34;../pics/intcalib/c1_dist.jpg&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt=&#34;undistorted&#34; src=&#34;../pics/intcalib/c1_undist.jpg&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;For more undistortion options, see &lt;code&gt;help ocam_undistort_map&lt;/code&gt;. In
particular setting &lt;code&gt;OutputView&lt;/code&gt; to &lt;code&gt;same&lt;/code&gt; is a useful choice, but you
can also increase the resolution to reduce quality loss during
undistort. Note that the intrinsic matrix $\cc{\mvec{K}}$ depends on the undistortion
mode used, and is in fact a field of the undistortion structure:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nohighlight&#34;&gt;&amp;gt;&amp;gt; u
u = 
    map: [2073600x2 double]
      K: [3x3 double]
    res: [1920 1080]
&amp;gt;&amp;gt; u.K&#39;
ans =
          472.402621799665                         0          958.773771858922
                         0          476.268511742695          539.250755510546
                         0                         0                         1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If instead &lt;code&gt;OutputView&lt;/code&gt; &lt;code&gt;same&lt;/code&gt; is used, the focal lengths will increase substantially!&lt;/p&gt;

&lt;h2 id=&#34;matlab-toolbox-tango-bottom-and-vi-sensors&#34;&gt;Matlab Toolbox: Tango Bottom and VI Sensors&lt;/h2&gt;

&lt;p&gt;We use Matlab&amp;rsquo;s built-in camera calibration tool, which uses the
camera model proposed by &lt;a href=&#34;http://www.vision.caltech.edu/bouguetj/calib_doc/&#34; title=&#34;Bouguet, J. Y.: Camera Calibration Toolbox for Matlab. Computational Vision at the California Institute of Technology.&#34;&gt;Bouguet&lt;/a&gt; for the CalTec Camera Calibration
Toolbox. The model consists of a perspective projection followed by a
radial distortion. For parsimony, we did not allow for skew or
tangential distortion, and limited the number of radial distortion
coefficients to two. This leads to the following model: the 3D point
$\cc{\cvec{X}{W}}$ in world coordinates is first transformed to the camera
frame by the current camera pose $\cc{\ctrans{T}{W}{C}}$:
$$
\cc{\vvec{x}{y}{z}=\ \cvec{X}{C}=\ \ctrans{T}{W}{C}\ \cvec{X}{W}}.
$$
Then, the projected 2D coordinates are obtained via:
$$
\cc{\vvt{x&amp;rsquo;}{y&amp;rsquo;} =\ \vvt{x/z}{y/z}}.
$$
Now the radial distortion is captured by:
$$
\cc{\vvt{x\dp}{y\dp} =\ \vvt{x&amp;rsquo;(1+k_1r&amp;rsquo;^2 +k_2r&amp;rsquo;^4)}{y&amp;rsquo;(1+k_1r&amp;rsquo;^2 +k_2r&amp;rsquo;^4)}},
$$
where $\cc{r&amp;rsquo;^2 = x&amp;rsquo;^2 + y&amp;rsquo;^2}$. Lastly, using the intrinsic matrix yields the sensor pixel coordinates:
$$
\cc{\vvt{u}{v} = \begin{bmatrix}f_x &amp;amp; 0 &amp;amp; c_x\\ 0 &amp;amp; f_y &amp;amp; c_y\end{bmatrix}\vvec{x\dp}{y\dp}{1}}.
$$&lt;/p&gt;

&lt;p&gt;Note that the camera models as stored in the Matlab cell array (see &lt;a href=&#34;#foo&#34;&gt;above&lt;/a&gt;) follow
Matlab convention, so their intrinsic matrix is transposed compared to the table below!
The VI sensor calibration is based off the rectified images (this explains why the radial distortion is virtually zero),
and is therefore the correct calibration to use for the rectified video frames in the ROS bag.&lt;/p&gt;

&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;Camera&lt;/td&gt;&lt;td&gt;$\cc{\begin{bmatrix}f_x &amp; 0 &amp; c_x\\\ 0 &amp; f_y &amp; c_y\end{bmatrix}}$&lt;/td&gt;&lt;td&gt;$\cc{k_1}$&lt;/td&gt;&lt;td&gt;$\cc{k_2}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Tango Bottom RGB&lt;/td&gt;&lt;td&gt;$\cc{\calmat{   1959.84}{   1959.39}{    981.87}{    524.94}}$&lt;/td&gt;&lt;td&gt; 0.21253&lt;/td&gt;&lt;td&gt;-0.46023&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;VI Sensor Left&lt;/td&gt;&lt;td&gt;$\cc{\calmat{    445.80}{    445.15}{    371.50}{    237.33}}$&lt;/td&gt;&lt;td&gt;-0.03671&lt;/td&gt;&lt;td&gt; 0.05260&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;VI Sensor Right&lt;/td&gt;&lt;td&gt;$\cc{\calmat{    445.75}{    445.23}{    369.28}{    238.72}}$&lt;/td&gt;&lt;td&gt;-0.03427&lt;/td&gt;&lt;td&gt; 0.04858&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&#34;fisheye-intrinsic-calibration-using-ocamcalib-toolbox&#34;&gt;Fisheye Intrinsic Calibration using OCamCalib Toolbox&lt;/h2&gt;

&lt;p&gt;We used the &lt;a href=&#34;https://sites.google.com/site/scarabotix/ocamcalib-toolbox/&#34; title=&#34;Scaramuzza, D: OCamCalib: Omnidirectional Camera Calibration Toolbox for Matlab.&#34;&gt;OCamCalib Toolbox&lt;/a&gt; (version 3.0) to fit a 4-parameter
polynomial to the forward projection function. The image center was
held fixed for parsimony, and because good results (less than 1px
average reprojection error) were obtained already without allowing the
center to float.&lt;/p&gt;

&lt;p&gt;Here the forward projection function of the GoPro Hero 4 camera
C2. Note that the horizontal FOV is twice the angle covered in the
graph (which is about 60 degrees).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../pics/intcalib/c2_projfun.jpg&#34; alt=&#34;forward projection function&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here a graphical illustration of the different extrinsic positions that
were assumed, showing that we took images with the calibration target as
far in the corner as possible:
&lt;img src=&#34;../pics/intcalib/c2_ext.jpg&#34; alt=&#34;external calibration positions&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Below are the calibration parameters obtained from the toolbox. Refer
to &lt;a href=&#34;https://sites.google.com/site/scarabotix/ocamcalib-toolbox/&#34; title=&#34;Scaramuzza, D: OCamCalib: Omnidirectional Camera Calibration Toolbox for Matlab.&#34;&gt;2&lt;/a&gt; for description of the parameters and the camera model. The easiest way to use these numbers is
by loading the calibration data into Matlab as described &lt;a href=&#34;#howtousecalibmodels&#34;&gt;here&lt;/a&gt;. Note that there is no intrinsic camera matrix $\cc{\mvec{K}}$ in this table since $\cc{\mvec{K}}$ depends on the way in which the &lt;a href=&#34;#undistfisheye&#34;&gt;undistortion&lt;/a&gt; is done.&lt;/p&gt;

&lt;table&gt;
&lt;tr&gt;&lt;td&gt;Camera&lt;/td&gt;&lt;td&gt;width&lt;/td&gt;&lt;td&gt;height&lt;/td&gt;&lt;td&gt;xc&lt;/td&gt;&lt;td&gt;yc&lt;/td&gt;&lt;td&gt;c&lt;/td&gt;&lt;td&gt;d&lt;/td&gt;&lt;td&gt;e&lt;/td&gt;&lt;td&gt;ss [polynomial coefficients]&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;GoPro C1&lt;/td&gt;&lt;td&gt;1920&lt;/td&gt;&lt;td&gt;1080&lt;/td&gt;&lt;td&gt;540&lt;/td&gt;&lt;td&gt;960&lt;/td&gt;&lt;td&gt;1.008&lt;/td&gt;&lt;td&gt;$2.710\times 10^{-4}$&lt;/td&gt;&lt;td&gt;$2.158\times 10^{-4}$&lt;/td&gt;&lt;td&gt;$\cc{\begin{bmatrix}-867.43&amp;0&amp;3.113\times 10^{-4}&amp;5.142\times 10^{-8}&amp;2.253\times 10^{-11}\end{bmatrix}}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;GoPro C2&lt;/td&gt;&lt;td&gt;1920&lt;/td&gt;&lt;td&gt;1080&lt;/td&gt;&lt;td&gt;540&lt;/td&gt;&lt;td&gt;960&lt;/td&gt;&lt;td&gt;1.004&lt;/td&gt;&lt;td&gt;$2.989\times 10^{-4}$&lt;/td&gt;&lt;td&gt;$0.921\times 10^{-3}$&lt;/td&gt;&lt;td&gt;$\cc{\begin{bmatrix}-877.47&amp;0&amp;3.339\times 10^{-4}&amp;6.175\times 10^{-9}&amp;1.104\times 10^{-11}\end{bmatrix}}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;GoPro C3&lt;/td&gt;&lt;td&gt;1920&lt;/td&gt;&lt;td&gt;1080&lt;/td&gt;&lt;td&gt;540&lt;/td&gt;&lt;td&gt;960&lt;/td&gt;&lt;td&gt;1.006&lt;/td&gt;&lt;td&gt;$1.794\times 10^{-4}$&lt;/td&gt;&lt;td&gt;$5.722\times 10^{-4}$&lt;/td&gt;&lt;td&gt;$\cc{\begin{bmatrix}-875.98&amp;0&amp;3.358\times 10^{-4}&amp;2.055\times 10^{-8}&amp;2.877\times 10^{-11}\end{bmatrix}}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Tango Top Fisheye&lt;/td&gt;&lt;td&gt;640&lt;/td&gt;&lt;td&gt;480&lt;/td&gt;&lt;td&gt;240&lt;/td&gt;&lt;td&gt;320&lt;/td&gt;&lt;td&gt;1.000&lt;/td&gt;&lt;td&gt;$4.162\times 10^{-4}$&lt;/td&gt;&lt;td&gt;$1.303\times 10^{-4}$&lt;/td&gt;&lt;td&gt;$\cc{\begin{bmatrix}-273.59&amp;0&amp;1.292\times 10^{-3}&amp;5.874\times 10^{-7}&amp;2.741\times 10^{-9}\end{bmatrix}}$&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Extrinsic Calibration</title>
      <link>https://daniilidis-group.github.io/penncosyvio/extrinsic_calib/</link>
      <pubDate>Wed, 09 Mar 2016 00:11:02 +0100</pubDate>
      
      <guid>https://daniilidis-group.github.io/penncosyvio/extrinsic_calib/</guid>
      <description>

&lt;h2 id=&#34;rig-and-coordinate-system-conventions&#34;&gt;Rig and Coordinate System Conventions&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;../pics/rig.jpg&#34; width=&#34;500&#34;&gt;
&lt;p&gt;All sensors are extrinsically calibrated with respect to the rig body (B) reference frame (which coincides with the center GoPro C2 camera).
For each sensor we give the transform $\cc{\mvec{T}}$ from the rig coordinate system to the sensor coordinate system. A left superscript indicates the coordinate system in which vectors are expressed, i.e. $\cc{\cvec{X}{C1}}$ are the coordinates of vector $\cc{X}$ in the reference frame of camera C1. The transform $\cc{\ctrans{T}{S1}{S2}}$ takes a vector expressed in coordinate system S1 and transforms it to system S2: $\cc{\cvec{X}{S2}=\ctrans{T}{S1}{S2}\ \cvec{X}{S1}}$. For example, given coordinates in the rig reference frame (B), the ones for camera C1 can be obtained via:
$$
\cc{\cvec{X}{C1}=\ctrans{T}{B}{C1}\ \cvec{X}{B}}
$$.&lt;/p&gt;

&lt;h2 id=&#34;transformations-in-matrix-form&#34;&gt;Transformations in Matrix Form&lt;/h2&gt;

&lt;p&gt;In the below table, the transform T can be expressed as a 3x4 transformation SE(3) matrix:
$$
\cc{\mvec{T}=\begin{bmatrix}\mvec{R}\ \mvec{t}\end{bmatrix}}
$$
where $\cc{\mvec{t}}$ is a 3x1 translation vector, and $\cc{\mvec{R}}$ is a 3x3 SO(3) rotation.
Note that $\cc{\mvec{t}}$ is expressed in meters, and is given in the coordinate system of the sensor.
&lt;/p&gt;

&lt;table&gt;
&lt;tr&gt;&lt;td&gt;Sensor&lt;/td&gt;&lt;td&gt;Transform&lt;/td&gt;&lt;td align=&#34;center&#34;&gt;$\cc{\begin{bmatrix}\mvec{R}\ \mvec{t}\end{bmatrix}}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;GoPro C1&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{C1}}$&lt;/td&gt;&lt;td&gt;$\cc{\begin{bmatrix}    0.2961304418 &amp;    0.0011052645 &amp;    0.9551468682 &amp;    0.0827\\     0.0191451276 &amp;    0.9997915577 &amp;   -0.0070926152 &amp;    0.0025\\    -0.9549556144 &amp;    0.0203867479 &amp;    0.2960475553 &amp;   -0.1273\\ \end{bmatrix}}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;GoPro C2&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{C2}}$&lt;/td&gt;&lt;td&gt;$\cc{\begin{bmatrix}    1.0000000000 &amp;    0.0000000000 &amp;    0.0000000000 &amp;    0.0000\\     0.0000000000 &amp;    1.0000000000 &amp;    0.0000000000 &amp;    0.0000\\     0.0000000000 &amp;    0.0000000000 &amp;    1.0000000000 &amp;    0.0000\\ \end{bmatrix}}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;GoPro C3&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{C3}}$&lt;/td&gt;&lt;td&gt;$\cc{\begin{bmatrix}    0.2251883426 &amp;   -0.0049445900 &amp;   -0.9743027052 &amp;   -0.0396\\     0.0047482257 &amp;    0.9999808169 &amp;   -0.0039774600 &amp;   -0.0008\\     0.9743036820 &amp;   -0.0037305315 &amp;    0.2252075009 &amp;   -0.1570\\ \end{bmatrix}}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Tango Bottom RGB&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{TBR}}$&lt;/td&gt;&lt;td&gt;$\cc{\begin{bmatrix}    0.9986085337 &amp;    0.0027720771 &amp;   -0.0526622452 &amp;   -0.0684\\    -0.0079486091 &amp;    0.9951207697 &amp;   -0.0983436495 &amp;    0.0841\\     0.0521326778 &amp;    0.0986253992 &amp;    0.9937581268 &amp;    0.0440\\ \end{bmatrix}}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Tango Top Fisheye&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{TTF}}$&lt;/td&gt;&lt;td&gt;$\cc{\begin{bmatrix}    0.9995000221 &amp;    0.0130284988 &amp;   -0.0288090975 &amp;   -0.0147\\    -0.0128514581 &amp;    0.9998974312 &amp;    0.0063219540 &amp;    0.2082\\     0.0288885081 &amp;   -0.0059485542 &amp;    0.9995649398 &amp;   -0.0017\\ \end{bmatrix}}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;VI Sensor Left&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{VL}}$&lt;/td&gt;&lt;td&gt;$\cc{\begin{bmatrix}    0.9991687124 &amp;   -0.0059124048 &amp;   -0.0403351908 &amp;    0.0427\\     0.0050322682 &amp;    0.9997477793 &amp;   -0.0218873038 &amp;   -0.1168\\     0.0404544240 &amp;    0.0216661317 &amp;    0.9989464542 &amp;   -0.0199\\ \end{bmatrix}}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;VI Sensor Right&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{VR}}$&lt;/td&gt;&lt;td&gt;$\cc{\begin{bmatrix}    0.9990911633 &amp;   -0.0072415591 &amp;   -0.0420048470 &amp;   -0.0675\\     0.0063429185 &amp;    0.9997489915 &amp;   -0.0214877011 &amp;   -0.1165\\     0.0421499079 &amp;    0.0212017389 &amp;    0.9988863156 &amp;   -0.0201\\ \end{bmatrix}}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Tango Bottom IMU&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{TBI}}$&lt;/td&gt;&lt;td&gt;$\cc{\begin{bmatrix}    0.9986085337 &amp;    0.0027720770 &amp;   -0.0526622452 &amp;   -0.0070\\     0.0192066304 &amp;   -0.9491479460 &amp;    0.3142439848 &amp;   -0.0740\\    -0.0491131534 &amp;   -0.3148181892 &amp;   -0.9478804808 &amp;   -0.0588\\ \end{bmatrix}}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Tango Top IMU&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{TTI}}$&lt;/td&gt;&lt;td&gt;$\cc{\begin{bmatrix}    0.9995000221 &amp;    0.0130284988 &amp;   -0.0288090975 &amp;   -0.0033\\     0.0200390427 &amp;   -0.9658149773 &amp;    0.2584567012 &amp;   -0.2033\\    -0.0244569550 &amp;   -0.2589047853 &amp;   -0.9655931698 &amp;   -0.0498\\ \end{bmatrix}}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;VI Sensor IMU&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{VI}}$&lt;/td&gt;&lt;td&gt;$\cc{\begin{bmatrix}    0.9989551739 &amp;   -0.0016800380 &amp;   -0.0456698809 &amp;   -0.0310\\     0.0005817624 &amp;    0.9997105684 &amp;   -0.0240508012 &amp;   -0.1257\\     0.0456970689 &amp;    0.0239991032 &amp;    0.9986670221 &amp;   -0.0171\\ \end{bmatrix}}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Tango Bottom Pose&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{TBP}}$&lt;/td&gt;&lt;td&gt;$\cc{\begin{bmatrix}    0.9986085337 &amp;    0.0027720770 &amp;   -0.0526622452 &amp;   -0.0070\\     0.0192066304 &amp;   -0.9491479460 &amp;    0.3142439848 &amp;   -0.0740\\    -0.0491131534 &amp;   -0.3148181892 &amp;   -0.9478804808 &amp;   -0.0588\\ \end{bmatrix}}$&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&#34;transformations-in-quaternion-form&#34;&gt;Transformations in Quaternion Form&lt;/h2&gt;

&lt;p&gt;The rotation matrix $\cc{\mvec{R}}$ is related to the unit
quaternion $\mq{q_w}{q_x}{q_y}{q_z}$ via:
$$
\cc{
\mathbf{R} = {\begin{pmatrix}
1 -2q_y^2-2q_z^2 &amp;amp; 2q_xq_y - 2q_zq_w &amp;amp; 2q_xq_z + 2q_yq_w\\
2q_xq_y + 2q_zq_w &amp;amp; 1 - 2q_x^2 -2 q_z^2 &amp;amp; 2q_yq_z -2 q_xq_w\\
2q_xq_z -2 q_yq_w &amp;amp; 2q_yq_z + 2 q_xq_w &amp;amp; 1-2q_x^2-2q_y^2\\
\end{pmatrix}}}
$$&lt;/p&gt;

&lt;table&gt;
&lt;tr&gt;&lt;td&gt;Sensor&lt;/td&gt;&lt;td&gt;Trans&lt;/td&gt;&lt;td align=&#34;center&#34;&gt;$\mq{q_w}{q_x}{q_y}{q_z}$&lt;/td&gt;&lt;td&gt;$\cc{\mvec{t}}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;GoPro C1&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{C1}}$&lt;/td&gt;&lt;td&gt;$\mq{    0.8049797443}{    0.0085341784}{    0.5932144554}{    0.0056025829}$&lt;/td&gt;&lt;td&gt;$\nvec{    0.0827}{    0.0025}{   -0.1273}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;GoPro C2&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{C2}}$&lt;/td&gt;&lt;td&gt;$\mq{    1.0000000000}{    0.0000000000}{    0.0000000000}{    0.0000000000}$&lt;/td&gt;&lt;td&gt;$\nvec{    0.0000}{    0.0000}{    0.0000}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;GoPro C3&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{C3}}$&lt;/td&gt;&lt;td&gt;$\mq{    0.7826839497}{    0.0000788724}{   -0.6224116350}{    0.0030960184}$&lt;/td&gt;&lt;td&gt;$\nvec{   -0.0396}{   -0.0008}{   -0.1570}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Tango Bottom RGB&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{TBR}}$&lt;/td&gt;&lt;td&gt;$\mq{    0.9984347037}{    0.0493194617}{   -0.0262398038}{   -0.0026843734}$&lt;/td&gt;&lt;td&gt;$\nvec{   -0.0684}{    0.0841}{    0.0440}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Tango Top Fisheye&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{TTF}}$&lt;/td&gt;&lt;td&gt;$\mq{    0.9998702907}{   -0.0030680250}{   -0.0144262726}{   -0.0064708286}$&lt;/td&gt;&lt;td&gt;$\nvec{   -0.0147}{    0.2082}{   -0.0017}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;VI Sensor Left&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{VL}}$&lt;/td&gt;&lt;td&gt;$\mq{    0.9997328325}{    0.0108912687}{   -0.0202028012}{    0.0027368995}$&lt;/td&gt;&lt;td&gt;$\nvec{    0.0427}{   -0.1168}{   -0.0199}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;VI Sensor Right&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{VR}}$&lt;/td&gt;&lt;td&gt;$\mq{    0.9997157684}{    0.0106753943}{   -0.0210446703}{    0.0033970850}$&lt;/td&gt;&lt;td&gt;$\nvec{   -0.0675}{   -0.1165}{   -0.0201}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Tango Bottom IMU&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{TBI}}$&lt;/td&gt;&lt;td&gt;$\mq{   -0.1593581712}{    0.9868684006}{    0.0055677908}{   -0.0257824140}$&lt;/td&gt;&lt;td&gt;$\nvec{   -0.0070}{   -0.0740}{   -0.0588}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Tango Top IMU&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{TTI}}$&lt;/td&gt;&lt;td&gt;$\mq{   -0.1304720995}{    0.9913259011}{    0.0083392206}{   -0.0134330326}$&lt;/td&gt;&lt;td&gt;$\nvec{   -0.0033}{   -0.2033}{   -0.0498}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;VI Sensor IMU&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{VI}}$&lt;/td&gt;&lt;td&gt;$\mq{    0.9996665399}{    0.0120164831}{   -0.0228493568}{    0.0005656387}$&lt;/td&gt;&lt;td&gt;$\nvec{   -0.0310}{   -0.1257}{   -0.0171}$&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Tango Bottom Pose&lt;/td&gt;&lt;td&gt;$\cc{\ctrans{T}{B}{TBP}}$&lt;/td&gt;&lt;td&gt;$\mq{   -0.1593581712}{    0.9868684006}{    0.0055677908}{   -0.0257824140}$&lt;/td&gt;&lt;td&gt;$\nvec{   -0.0070}{   -0.0740}{   -0.0588}$&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&#34;extrinsic-calibration-method&#34;&gt;Extrinsic calibration method&lt;/h2&gt;

&lt;p&gt;The AprilTags on the north wall of the Singh Center were used as a calibration target. A total of 16 synchronized snapshots were cut from the video footage, and optical calibration was performed as described in the &lt;a href=&#34;docs/penncosyvio.pdf&#34;&gt;ICRA paper&lt;/a&gt;. As an example, here is a picture of snapshot #4. The complete set of images is available for download, as well as the locations of the AprilTags in that area.
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;figure&gt;&lt;img src=&#34;../pics/extcalib/ec_vl.jpg&#34; width=&#34;500&#34;&gt;&lt;figcaption&gt;VI Sensor left&lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;&lt;figure&gt;&lt;img src=&#34;../pics/extcalib/ec_vr.jpg&#34; width=&#34;500&#34;&gt;&lt;figcaption&gt;VI Sensor right&lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;figure&gt;&lt;img src=&#34;../pics/extcalib/ec_tb.jpg&#34; width=&#34;500&#34;&gt;&lt;figcaption&gt;Tango Bottom&lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;&lt;figure&gt;&lt;img src=&#34;../pics/extcalib/ec_tt.jpg&#34; width=&#34;500&#34;&gt;&lt;figcaption&gt;Tango Top&lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;figure&gt;&lt;img src=&#34;../pics/extcalib/ec_c1.jpg&#34; width=&#34;500&#34;&gt;&lt;figcaption&gt;GoPro C1&lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;&lt;figure&gt;&lt;img src=&#34;../pics/extcalib/ec_c2.jpg&#34; width=&#34;500&#34;&gt;&lt;figcaption&gt;GoPro C2&lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;figure&gt;&lt;img src=&#34;../pics/extcalib/ec_c3.jpg&#34; width=&#34;500&#34;&gt;&lt;figcaption&gt;GoPro C3&lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;/p&gt;

&lt;h2 id=&#34;notes-on-the-vi-sensor-calibration&#34;&gt;Notes on the VI sensor calibration&lt;/h2&gt;

&lt;p&gt;Although the VI-sensor has recorded the rectified images, they are in
fact &lt;em&gt;not&lt;/em&gt; rectified due to poor stereo calibration of the sensor. You
will need to use the extrinsic rectification to get the proper
camera-camera calibration. Alternatively, you can download a separate
right/left image extrinsic + intrinsic calibration (done with Kalibr)
from &lt;a href=&#34;../docs/visensor_calib.zip&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>